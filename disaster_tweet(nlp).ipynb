{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df=pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')\n",
    "sample_df=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train_df\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "train_df['keyword'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['location'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test_df\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['keyword'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['location'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test_df\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.pie(train_df['target'].value_counts(), labels=['Non-disaster', 'Disaster'], autopct='%0.2f')\n",
    "plt.legend()  # Adds a legend\n",
    "plt.title('Distribution of Disaster Tweets')  # Adds a descriptive title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random state\n",
    "random_state = 4041\n",
    "\n",
    "# import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# concat all the text for each labels\n",
    "non_disaster_text = [''.join(t) for t in train_df[train_df['target']==0]['text']]\n",
    "non_disaster_strings = ' '.join(map(str, non_disaster_text))\n",
    "disaster_text = [''.join(t) for t in train_df[train_df['target']==1]['text']]\n",
    "disaster_strings = ' '.join(map(str, disaster_text))\n",
    "\n",
    "# generate word clouds\n",
    "non_disaster_cloud = WordCloud(width=800, height=400, max_words=500, background_color='white', random_state=random_state).generate(non_disaster_strings)\n",
    "disaster_cloud = WordCloud(width=800, height=400, max_words=500, random_state=random_state).generate(disaster_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots for the generated clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize = (20,20))\n",
    "axes[0].imshow(non_disaster_cloud, interpolation='bilinear')\n",
    "axes[1].imshow(disaster_cloud, interpolation='bilinear')\n",
    "\n",
    "# turn the axis off\n",
    "[ax.axis('off') for ax in axes]\n",
    "\n",
    "# add titles\n",
    "axes[0].set_title('Non-disaster Tweets', fontsize=16)\n",
    "axes[1].set_title('Disaster Tweets', fontsize=16)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-remove urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    text = re.sub(r'((?:https?|ftp|file)://[-\\w\\d+=&@#/%?~|!:;\\.,]*)', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text'].apply(remove_url)\n",
    "test_df['text_cleaned'] = test_df['text'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_HTML(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_HTML)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rmove Characters References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(text):\n",
    "    text = re.sub(r'&[a-zA-Z]+;?', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_references)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Non-printable Characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.printable\n",
    "\n",
    "def remove_non_printable(text):\n",
    "    text = ''.join([word for word in text if word in string.printable])\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_non_printable)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_non_printable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Numeric Values\n",
    "Remove numeric values, including mixtures of alphabetical characters and numeric values such as 'M194', '5km'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_num)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_num)\n",
    "\n",
    "train_df.tail()\n",
    "\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"# Feature Engineering\n",
    "Below are 10 features we're going to create:\n",
    "\n",
    "- Number of sentences\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Number of hashtags\n",
    "- Number of mentions\n",
    "- Number of all caps words\n",
    "- Average length of words\n",
    "- Number of proper nouns (PROPN)\n",
    "- Number of non-proper nouns (NOUN)\n",
    "- Percentage of characters that are punctuation\n",
    "\n",
    "# Number of SentencesÂ¶\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# create a new feature for the number of sentences in each Tweet\n",
    "train_df['sent_count'] = train_df['text'].apply(nltk.tokenize.sent_tokenize).apply(len)\n",
    "test_df['sent_count'] = test_df['text'].apply(nltk.tokenize.sent_tokenize).apply(len)\n",
    "\n",
    "# create a new feature for the number of words\n",
    "train_df['word_count'] = train_df['text'].apply(nltk.tokenize.word_tokenize).apply(len)\n",
    "test_df['word_count'] = test_df['text'].apply(nltk.tokenize.word_tokenize).apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature for the number of characters excluding white spaces\n",
    "train_df['char_count'] = train_df['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "test_df['char_count'] = test_df['text'].apply(lambda x: len(x) - x.count(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns the number of hashtags in a string\n",
    "def hash_count(string):\n",
    "    words = string.split()\n",
    "    hashtags = [w for w in words if w.startswith('#')]\n",
    "    return len(hashtags)\n",
    "\n",
    "# create a new feature for the number of hashtags\n",
    "train_df['hash_count'] = train_df['text'].apply(hash_count)\n",
    "test_df['hash_count'] = test_df['text'].apply(hash_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns the number of mentions in a string\n",
    "def ment_count(string):\n",
    "    words = string.split()\n",
    "    mentions = [w for w in words if w.startswith('@')]\n",
    "    return len(mentions)\n",
    "\n",
    "# create a new feature for the number of mentions\n",
    "train_df['ment_count'] = train_df['text'].apply(ment_count)\n",
    "test_df['ment_count'] = test_df['text'].apply(ment_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of All Caps Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def all_caps_count(string):\n",
    "    words = string.split()\n",
    "    pattern = re.compile(r'\\b[A-Z]{2,}\\b')  # Matches words with 2 or more consecutive uppercase letters\n",
    "    caps_words = [word for word in words if pattern.fullmatch(word)]\n",
    "    return len(caps_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Length of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns the average length of words\n",
    "def avg_word_len(string):\n",
    "    words = string.split()\n",
    "    total_len = sum([len(words[i]) for i in range(len(words))])\n",
    "    avg_len = round(total_len / len(words), 2)\n",
    "    return avg_len\n",
    "\n",
    "# create a new feature for the average length of words\n",
    "train_df['avg_word_len'] = train_df['text'].apply(avg_word_len)\n",
    "test_df['avg_word_len'] = test_df['text'].apply(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# define a function using nltk that returns the number of proper nouns in the text\n",
    "def propn_count_nltk(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = [token for token in nltk.pos_tag(tokens)]\n",
    "    propn_count = len([token for (token, tag) in tagged if tag == 'NNP' or tag == 'NNPS'])\n",
    "    return propn_count\n",
    "\n",
    "\n",
    "# create a new feature for the number of proper nouns\n",
    "train_df['propn_count_nltk'] = train_df['text'].apply(propn_count_nltk)\n",
    "test_df['propn_count_nltk'] = test_df['text'].apply(propn_count_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned', 'propn_count_nltk']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test how nltk worked with the first text\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token, tag) for (token, tag) in nltk.pos_tag(nltk.word_tokenize(string)) if tag == 'NNP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test how nltk works with the first text after lowercasing it\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token, tag) for (token, tag) in nltk.pos_tag(nltk.word_tokenize(string.lower())) if tag == 'NNP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# check the same string with spaCy\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token.text, token.pos_) for token in nlp(string) if token.pos_=='PROPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns number of proper nouns with spaCy\n",
    "def propn_count(text, model=nlp):\n",
    "    doc = model(text)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "# create a new feature for numbers of proper nouns\n",
    "train_df['propn_count'] = train_df['text'].apply(propn_count)\n",
    "test_df['propn_count'] = test_df['text'].apply(propn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'propn_count_nltk' columns\n",
    "train_df = train_df.drop(['propn_count_nltk'], axis=1)\n",
    "test_df = test_df.drop(['propn_count_nltk'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned', 'propn_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Non-proper Nouns (NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns number of non-proper nouns\n",
    "def noun_count(text, model=nlp):\n",
    "    doc = model(text)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "# create a new feature for numbers of non-proper nouns\n",
    "train_df['noun_count'] = train_df['text'].apply(noun_count)\n",
    "test_df['noun_count'] = test_df['text'].apply(noun_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of Characters that are Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# define a function that returns the percentage of punctuation\n",
    "def punc_per(text):\n",
    "    total_count = len(text) - text.count(\" \")\n",
    "    punc_count = sum([1 for c in text if c in string.punctuation])\n",
    "    if punc_count != 0 and total_count != 0:\n",
    "        return round(punc_count / total_count * 100, 2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# create a new feature for the percentage of punctuation in text\n",
    "train_df['punc_per'] = train_df['text'].apply(punc_per)\n",
    "test_df['punc_per'] = test_df['text'].apply(punc_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the results\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "test_df.tail()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
